{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=dataset_train.iloc[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>325.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>331.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>329.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>328.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>322.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>313.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>310.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>314.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>311.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>314.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>312.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>319.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>294.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>291.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>292.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>287.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>284.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>284.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>287.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>290.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>291.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>291.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>294.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>296.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>302.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>303.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>304.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>302.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>304.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>304.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>766.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>771.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>762.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>772.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>767.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>764.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>760.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>771.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>770.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>757.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>744.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>757.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>764.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>761.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>772.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>780.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>785.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>793.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>797.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>797.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>800.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>790.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>796.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>795.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>792.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>790.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>790.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>793.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>783.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>782.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open\n",
       "0     325.25\n",
       "1     331.27\n",
       "2     329.83\n",
       "3     328.34\n",
       "4     322.04\n",
       "5     313.70\n",
       "6     310.59\n",
       "7     314.43\n",
       "8     311.96\n",
       "9     314.81\n",
       "10    312.14\n",
       "11    319.30\n",
       "12    294.16\n",
       "13    291.91\n",
       "14    292.07\n",
       "15    287.68\n",
       "16    284.92\n",
       "17    284.32\n",
       "18    287.95\n",
       "19    290.41\n",
       "20    291.38\n",
       "21    291.34\n",
       "22    294.23\n",
       "23    296.39\n",
       "24    302.44\n",
       "25    303.18\n",
       "26    304.87\n",
       "27    302.81\n",
       "28    304.11\n",
       "29    304.63\n",
       "...      ...\n",
       "1228  766.92\n",
       "1229  771.37\n",
       "1230  762.61\n",
       "1231  772.63\n",
       "1232  767.73\n",
       "1233  764.26\n",
       "1234  760.00\n",
       "1235  771.53\n",
       "1236  770.07\n",
       "1237  757.44\n",
       "1238  744.59\n",
       "1239  757.71\n",
       "1240  764.73\n",
       "1241  761.00\n",
       "1242  772.48\n",
       "1243  780.00\n",
       "1244  785.04\n",
       "1245  793.90\n",
       "1246  797.40\n",
       "1247  797.34\n",
       "1248  800.40\n",
       "1249  790.22\n",
       "1250  796.76\n",
       "1251  795.84\n",
       "1252  792.36\n",
       "1253  790.90\n",
       "1254  790.68\n",
       "1255  793.70\n",
       "1256  783.33\n",
       "1257  782.75\n",
       "\n",
       "[1258 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc=MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scaled=sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08581368],\n",
       "       [ 0.09701243],\n",
       "       [ 0.09433366],\n",
       "       ..., \n",
       "       [ 0.95725128],\n",
       "       [ 0.93796041],\n",
       "       [ 0.93688146]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "y_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train=np.reshape(X_train, (len(X_train), X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 18s 15ms/step - loss: 0.0511\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0072\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0055\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0051\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0050\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0045\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0046\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0043\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0045\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0042\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0042\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0038\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 19s 15ms/step - loss: 0.0040\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0038\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0040\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0038\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 21s 17ms/step - loss: 0.0036\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 21s 17ms/step - loss: 0.0035\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0033\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0037\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 21s 18ms/step - loss: 0.0036\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0036\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0034\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0035\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0041\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0030\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0028\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0030\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0031\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0030\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0028\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0033\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0029\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0029\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0026\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0029\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0029\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 21s 18ms/step - loss: 0.0029\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0030\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0029\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0025\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0026\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0025\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0025\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 21s 17ms/step - loss: 0.0026\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0024\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0024\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0022\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0023\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0023\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0023\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0021\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0019\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0017\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0023\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0019\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 20s 16ms/step - loss: 0.0017\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0019\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0018\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0019\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0022\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0020\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 19s 16ms/step - loss: 0.0019\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 20s 17ms/step - loss: 0.0017\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0017\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 21s 18ms/step - loss: 0.0017\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 38s 32ms/step - loss: 0.0016\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 32s 26ms/step - loss: 0.0017\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 25s 21ms/step - loss: 0.0017\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0017\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0018\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0017\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 25s 21ms/step - loss: 0.0015\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0014\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0014\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0015\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0016\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0015\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 23s 20ms/step - loss: 0.0015\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0015\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0014\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0014\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0015\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 25s 21ms/step - loss: 0.0015\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 27s 23ms/step - loss: 0.0015\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 24s 20ms/step - loss: 0.0015\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 22s 18ms/step - loss: 0.0014\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 22s 19ms/step - loss: 0.0014\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 23s 19ms/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24db5f50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "real_stock_price=dataset_test.iloc[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total=pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)\n",
    "inputs=dataset_total[len(dataset_total)-len(dataset_test)-60:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=inputs.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9299055 ],\n",
       "       [ 0.93113327],\n",
       "       [ 0.92750577],\n",
       "       [ 0.94415507],\n",
       "       [ 0.93876032],\n",
       "       [ 0.93403527],\n",
       "       [ 0.93483518],\n",
       "       [ 0.9313937 ],\n",
       "       [ 0.94636878],\n",
       "       [ 0.96569685],\n",
       "       [ 0.97510976],\n",
       "       [ 0.95966962],\n",
       "       [ 0.97808617],\n",
       "       [ 1.        ],\n",
       "       [ 0.98076494],\n",
       "       [ 0.97083116],\n",
       "       [ 0.98450406],\n",
       "       [ 0.96054394],\n",
       "       [ 0.9371419 ],\n",
       "       [ 0.92841729],\n",
       "       [ 0.90804747],\n",
       "       [ 0.8771858 ],\n",
       "       [ 0.92153434],\n",
       "       [ 0.93809063],\n",
       "       [ 0.93165414],\n",
       "       [ 0.95254483],\n",
       "       [ 0.88812412],\n",
       "       [ 0.88637547],\n",
       "       [ 0.87032145],\n",
       "       [ 0.88563137],\n",
       "       [ 0.90743359],\n",
       "       [ 0.91571173],\n",
       "       [ 0.89941588],\n",
       "       [ 0.91805566],\n",
       "       [ 0.9089404 ],\n",
       "       [ 0.9024853 ],\n",
       "       [ 0.89456061],\n",
       "       [ 0.91600938],\n",
       "       [ 0.9132934 ],\n",
       "       [ 0.88979835],\n",
       "       [ 0.86589404],\n",
       "       [ 0.89030062],\n",
       "       [ 0.90335962],\n",
       "       [ 0.89642086],\n",
       "       [ 0.91777662],\n",
       "       [ 0.93176576],\n",
       "       [ 0.94114145],\n",
       "       [ 0.95762334],\n",
       "       [ 0.96413424],\n",
       "       [ 0.96402262],\n",
       "       [ 0.96971501],\n",
       "       [ 0.95077759],\n",
       "       [ 0.96294367],\n",
       "       [ 0.96123223],\n",
       "       [ 0.95475854],\n",
       "       [ 0.95204256],\n",
       "       [ 0.95163331],\n",
       "       [ 0.95725128],\n",
       "       [ 0.93796041],\n",
       "       [ 0.93688146],\n",
       "       [ 0.92955205],\n",
       "       [ 0.94731751],\n",
       "       [ 0.94307612],\n",
       "       [ 0.96015329],\n",
       "       [ 0.98087655],\n",
       "       [ 0.98359253],\n",
       "       [ 0.97827219],\n",
       "       [ 0.98225314],\n",
       "       [ 0.98288563],\n",
       "       [ 0.98214153],\n",
       "       [ 0.979779  ],\n",
       "       [ 0.97849542],\n",
       "       [ 0.98182528],\n",
       "       [ 0.98245777],\n",
       "       [ 1.01045465],\n",
       "       [ 1.02407173],\n",
       "       [ 1.03930724],\n",
       "       [ 1.03354044],\n",
       "       [ 0.99624228],\n",
       "       [ 0.9631297 ]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[]\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.reshape(X_test, (len(X_test), X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04841759],\n",
       "       [-0.04841761],\n",
       "       [-0.0484176 ],\n",
       "       [-0.0484176 ],\n",
       "       [-0.04841759],\n",
       "       [-0.0484176 ],\n",
       "       [-0.0484176 ],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.0484176 ],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.04841759],\n",
       "       [-0.0484176 ],\n",
       "       [-0.04841761]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
